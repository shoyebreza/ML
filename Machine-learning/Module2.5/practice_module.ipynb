{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e73b48e6-c01a-4164-8ed1-e8a0e12a8fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   marks\n",
       "0     50\n",
       "1     60\n",
       "2     65\n",
       "3     70\n",
       "4     75\n",
       "5     80\n",
       "6     85\n",
       "7     90"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "marks = [50, 60, 65, 70, 75, 80, 85, 90]\n",
    "df = pd.DataFrame({'marks':marks})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95032f67-beae-4b1f-82d7-bc57964aa030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 71.875\n",
      "median: 72.5\n",
      "mode: 0    50\n",
      "1    60\n",
      "2    65\n",
      "3    70\n",
      "4    75\n",
      "5    80\n",
      "6    85\n",
      "7    90\n",
      "Name: marks, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "mean_value = df['marks'].mean()\n",
    "\n",
    "\n",
    "median_value = df['marks'].median()\n",
    "\n",
    "\n",
    "mode_value = df['marks'].mode()\n",
    "\n",
    "print(\"Mean:\", mean_value)\n",
    "print(\"median:\", median_value)\n",
    "print(\"mode:\", mode_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fb34387-15a9-4bc7-b432-327d077b6ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   temp\n",
       "0    29\n",
       "1    31\n",
       "2    33\n",
       "3    33\n",
       "4    32\n",
       "5    31\n",
       "6    30"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Temperatures = [29, 31, 33, 33, 32, 31, 30]\n",
    "temp = pd.DataFrame({\"temp\": Temperatures})\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b44112-4a60-4d90-8dd7-04d77db9c61f",
   "metadata": {},
   "source": [
    "variance = np.var(temp)\n",
    "std_deviation = np.std(temp)\n",
    "\n",
    "print(\"variance:\", variance)\n",
    "print(\"standard deviation:\", std_deviation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4159a1af-6ec6-49b0-8506-223ea239dcf1",
   "metadata": {},
   "source": [
    "## Part B – Percentiles, IQR and Z-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68c7f6ce-51c0-43ce-98c6-be6189bd07ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat_weights\n",
       "0          2.5\n",
       "1          3.0\n",
       "2          3.2\n",
       "3          3.3\n",
       "4          3.4\n",
       "5          3.5\n",
       "6          3.6\n",
       "7          3.9\n",
       "8          4.0\n",
       "9          4.5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_weights = [2.5, 3.0, 3.2, 3.3, 3.4, 3.5, 3.6, 3.9, 4.0, 4.5]\n",
    "\n",
    "cw = pd.DataFrame({'cat_weights': cat_weights})\n",
    "cw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "810e7774-93ed-4f86-924a-3b9670ea782a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25th percentile: 3.225\n",
      "50th percentile: 3.45\n",
      "75th percentile: 3.825\n"
     ]
    }
   ],
   "source": [
    "print(\"25th percentile:\", np.percentile(cw, 25))\n",
    "print(\"50th percentile:\", np.percentile(cw, 50))\n",
    "print(\"75th percentile:\", np.percentile(cw, 75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ec93f3-3c81-4dd6-a4f7-bc7c011fde8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IQR: 0.6000000000000001\n",
      "Lower bound: 2.325\n",
      "Upper bound: 4.7250000000000005\n",
      "Outlier Range: 2.325 to 4.7250000000000005\n"
     ]
    }
   ],
   "source": [
    "Q1 = np.percentile(cw, 25)\n",
    "Q3 = np.percentile(cw, 75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "print(\"IQR:\", IQR)\n",
    "print(\"Lower bound:\", lower_bound)\n",
    "print(\"Upper bound:\", upper_bound)\n",
    "print(\"Outlier Range:\", lower_bound, \"to\", upper_bound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a4b4779-bfff-47ca-a996-3b45012d3d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_scores: [-1.77 -0.88 -0.52 -0.34 -0.16  0.02  0.2   0.73  0.91  1.81]\n"
     ]
    }
   ],
   "source": [
    "mean = cw['cat_weights'].mean()\n",
    "std_dev = cw['cat_weights'].std()\n",
    "z_scores = [(x - mean) / std_dev for x in cw['cat_weights']]\n",
    "print('z_scores:', np.round(z_scores, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4760e028-96cc-4815-be3e-a9509318c0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Module 2 – Probability Basics for ML\n",
    "Part A – Basic Probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8633507-1370-494d-bc7c-54c4550e0527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(A) (Probability of liking pizza): 0.40\n",
      "P(B) (Probability of liking burgers): 0.50\n",
      "P(A intersect B) (Probability of liking both): 0.20\n",
      "P(A | B) (Probability of liking pizza given liking burgers): 0.40\n"
     ]
    }
   ],
   "source": [
    "total_people = 100\n",
    "likes_pizza = 40\n",
    "likes_burgers = 50\n",
    "likes_both = 20\n",
    "\n",
    "# P(A) - Probability of liking pizza\n",
    "P_A = likes_pizza / total_people\n",
    "\n",
    "# P(B) - Probability of liking burgers\n",
    "P_B = likes_burgers / total_people\n",
    "\n",
    "# P(A intersect B) - Probability of liking both\n",
    "P_A_intersect_B = likes_both / total_people\n",
    "\n",
    "# P(A | B) - Probability of liking pizza given liking burgers\n",
    "# P(A | B) = P(A intersect B) / P(B)\n",
    "if P_B != 0:\n",
    "    P_A_given_B = P_A_intersect_B / P_B\n",
    "else:\n",
    "    P_A_given_B = 0 # or handle error appropriately if P_B is 0\n",
    "\n",
    "print(f\"P(A) (Probability of liking pizza): {P_A:.2f}\")\n",
    "print(f\"P(B) (Probability of liking burgers): {P_B:.2f}\")\n",
    "print(f\"P(A intersect B) (Probability of liking both): {P_A_intersect_B:.2f}\")\n",
    "print(f\"P(A | B) (Probability of liking pizza given liking burgers): {P_A_given_B:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea00f67-52d5-410b-955c-b913649e64c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part B – Conditional Probability and Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a29f6d19-eacf-4953-af15-04e9af2872d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(S) (Probability of an email being spam): 0.10\n",
      "P(F | S) (Probability of containing 'free' given it is spam): 0.40\n",
      "P(F) (Probability of an email containing 'free'): 0.10\n",
      "P(S | F) (Probability of being spam given it contains 'free'): 0.40\n"
     ]
    }
   ],
   "source": [
    "total_emails = 1000\n",
    "spam_emails = 100\n",
    "spam_with_free = 40\n",
    "non_spam_with_free = 60\n",
    "\n",
    "# Calculate individual probabilities\n",
    "P_S = spam_emails / total_emails # P(Spam)\n",
    "P_F_given_S = spam_with_free / spam_emails # P(Free | Spam)\n",
    "\n",
    "# Total emails containing 'free'\n",
    "total_with_free = spam_with_free + non_spam_with_free\n",
    "P_F = total_with_free / total_emails # P(Free)\n",
    "\n",
    "# Apply Bayes' Theorem: P(S | F) = P(F | S) * P(S) / P(F)\n",
    "if P_F != 0:\n",
    "    P_S_given_F = (P_F_given_S * P_S) / P_F\n",
    "else:\n",
    "    P_S_given_F = 0 # Handle division by zero if P(F) is 0\n",
    "\n",
    "print(f\"P(S) (Probability of an email being spam): {P_S:.2f}\")\n",
    "print(f\"P(F | S) (Probability of containing 'free' given it is spam): {P_F_given_S:.2f}\")\n",
    "print(f\"P(F) (Probability of an email containing 'free'): {P_F:.2f}\")\n",
    "print(f\"P(S | F) (Probability of being spam given it contains 'free'): {P_S_given_F:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad6a1148-ea4a-4746-bb24-a9bcee928b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(D) (Probability of Disease): 0.00010\n",
      "P(+ | D) (Probability of positive test given Disease): 0.99\n",
      "P(- | not D) (Probability of negative test given no Disease): 0.99\n",
      "P(not D) (Probability of no Disease): 0.99990\n",
      "P(+ | not D) (False Positive Rate): 0.01\n",
      "P(+) (Overall probability of a positive test): 0.01010\n",
      "P(D | +) (Probability of Disease given a positive test): 0.00980\n"
     ]
    }
   ],
   "source": [
    "# Given probabilities\n",
    "P_D = 1 / 10000  # P(Disease)\n",
    "P_plus_given_D = 0.99 # P(+ | D) - Probability of positive test given Disease\n",
    "P_minus_given_notD = 0.99 # P(- | not D) - Probability of negative test given no Disease\n",
    "\n",
    "# Derived probabilities\n",
    "P_notD = 1 - P_D # P(not D) - Probability of no Disease\n",
    "P_plus_given_notD = 1 - P_minus_given_notD # P(+ | not D) - Probability of positive test given no Disease (False Positive Rate)\n",
    "\n",
    "# Calculate P(+) using the law of total probability\n",
    "P_plus = (P_plus_given_D * P_D) + (P_plus_given_notD * P_notD)\n",
    "\n",
    "# Apply Bayes' Theorem to find P(D | +)\n",
    "# P(D | +) = [P(+ | D) * P(D)] / P(+)\n",
    "if P_plus != 0:\n",
    "    P_D_given_plus = (P_plus_given_D * P_D) / P_plus\n",
    "else:\n",
    "    P_D_given_plus = 0 # Handle division by zero\n",
    "\n",
    "print(f\"P(D) (Probability of Disease): {P_D:.5f}\")\n",
    "print(f\"P(+ | D) (Probability of positive test given Disease): {P_plus_given_D:.2f}\")\n",
    "print(f\"P(- | not D) (Probability of negative test given no Disease): {P_minus_given_notD:.2f}\")\n",
    "print(f\"P(not D) (Probability of no Disease): {P_notD:.5f}\")\n",
    "print(f\"P(+ | not D) (False Positive Rate): {P_plus_given_notD:.2f}\")\n",
    "print(f\"P(+) (Overall probability of a positive test): {P_plus:.5f}\")\n",
    "print(f\"P(D | +) (Probability of Disease given a positive test): {P_D_given_plus:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcb5967-a07e-470d-a9ba-3bd35b7ac64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Part C – Confusion Matrix and Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d318c598-bdc8-4e1d-9216-0c78083ab3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.89 (89.00%)\n",
      "Precision = 0.60 (60.00%)\n",
      "Recall = 0.80 (80.00%)\n",
      "F1 Score = 0.69 (68.57%)\n",
      "Specificity = 0.91 (90.59%)\n",
      "Negative Predictive Value (NPV) = 0.96 (96.25%)\n",
      "Prevalence = 0.15 (15.00%)\n"
     ]
    }
   ],
   "source": [
    "# Given confusion matrix values\n",
    "TP = 120 # True Positives\n",
    "FN = 30  # False Negatives\n",
    "FP = 80  # False Positives\n",
    "TN = 770 # True Negatives\n",
    "\n",
    "# Calculate Total\n",
    "Total = TP + TN + FP + FN\n",
    "\n",
    "# Compute metrics\n",
    "Accuracy = (TP + TN) / Total\n",
    "Precision = TP / (TP + FP)\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "# Check for division by zero before calculating F1\n",
    "if (Precision + Recall) != 0:\n",
    "    F1 = 2 * Precision * Recall / (Precision + Recall)\n",
    "else:\n",
    "    F1 = 0.0 # Handle case where Precision and Recall are both 0\n",
    "\n",
    "Specificity = TN / (TN + FP)\n",
    "NPV = TN / (TN + FN)\n",
    "Prevalence = (TP + FN) / Total\n",
    "\n",
    "print(f\"Accuracy = {Accuracy:.2f} ({Accuracy*100:.2f}%)\")\n",
    "print(f\"Precision = {Precision:.2f} ({Precision*100:.2f}%)\")\n",
    "print(f\"Recall = {Recall:.2f} ({Recall*100:.2f}%)\")\n",
    "print(f\"F1 Score = {F1:.2f} ({F1*100:.2f}%)\")\n",
    "print(f\"Specificity = {Specificity:.2f} ({Specificity*100:.2f}%)\")\n",
    "print(f\"Negative Predictive Value (NPV) = {NPV:.2f} ({NPV*100:.2f}%)\")\n",
    "print(f\"Prevalence = {Prevalence:.2f} ({Prevalence*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2bceb95-248d-4bc6-9ee4-f705848786db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "  True Positives (TP): 0\n",
      "  False Negatives (FN): 200\n",
      "  False Positives (FP): 0\n",
      "  True Negatives (TN): 9800\n",
      "\n",
      "Accuracy = 0.9800 (98.00%)\n",
      "Precision = 0.0000 (0.00%)\n",
      "Recall = 0.0000 (0.00%)\n",
      "F1 Score = 0.0000 (0.00%)\n",
      "Specificity = 1.0000 (100.00%)\n",
      "\n",
      "Why is Accuracy misleading in this case?\n",
      "---------------------------------------\n",
      "Accuracy is calculated as (TP + TN) / Total. In this scenario, the model predicts everything as negative. Since 98% of the data are actual negatives, the model correctly predicts 9800 out of 10000 samples. This leads to an accuracy of 98%. However, the model completely fails to identify any of the positive samples (Recall = 0), making it useless for detecting the minority class. A high accuracy in such an imbalanced dataset can give a false sense of good performance.\n",
      "\n",
      "Which metric would be more appropriate for imbalanced data and why?\n",
      "------------------------------------------------------------------\n",
      "For imbalanced datasets, metrics that specifically evaluate the model's performance on the minority class are more appropriate. These include:\n",
      "1. Recall (Sensitivity): This measures the proportion of actual positive cases that are correctly identified. In this example, Recall is 0, correctly highlighting the model's inability to detect positives.\n",
      "2. Precision: This measures the proportion of positive predictions that are actually correct. In this example, Precision is 0, as there are no correct positive predictions.\n",
      "3. F1-Score: This is the harmonic mean of Precision and Recall. It provides a balanced measure of a model's performance on positive cases, especially when there's an uneven class distribution. An F1-Score of 0 in this case correctly indicates poor performance.\n",
      "4. Specificity: This measures the proportion of actual negative cases that are correctly identified. While high here, it doesn't tell us about positive identification.\n",
      "5. Area Under the Receiver Operating Characteristic (ROC-AUC) curve or Precision-Recall (PR-AUC) curve: These provide a comprehensive evaluation of a classifier's performance across various classification thresholds, which is very useful for imbalanced datasets.\n",
      "\n",
      "In this specific case, metrics like Recall, Precision, and F1-Score clearly show the model's complete failure to identify the positive class, unlike accuracy which suggests a high performance.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given data\n",
    "total_samples = 10000\n",
    "positive_percentage = 0.02\n",
    "\n",
    "# Calculate actual positive and negative samples\n",
    "actual_positives = int(total_samples * positive_percentage)\n",
    "actual_negatives = total_samples - actual_positives\n",
    "\n",
    "# Model predicts everything as negative\n",
    "TP = 0  # True Positives: Actual positive, predicted positive\n",
    "FN = actual_positives  # False Negatives: Actual positive, predicted negative\n",
    "FP = 0  # False Positives: Actual negative, predicted positive\n",
    "TN = actual_negatives  # True Negatives: Actual negative, predicted negative\n",
    "\n",
    "print(f\"Confusion Matrix:\")\n",
    "print(f\"  True Positives (TP): {TP}\")\n",
    "print(f\"  False Negatives (FN): {FN}\")\n",
    "print(f\"  False Positives (FP): {FP}\")\n",
    "print(f\"  True Negatives (TN): {TN}\\n\")\n",
    "\n",
    "# Compute metrics\n",
    "Total = TP + TN + FP + FN\n",
    "\n",
    "Accuracy = (TP + TN) / Total\n",
    "\n",
    "# Handle division by zero for Precision if TP + FP is 0\n",
    "Precision = TP / (TP + FP) if (TP + FP) != 0 else 0.0\n",
    "\n",
    "# Handle division by zero for Recall if TP + FN is 0\n",
    "Recall = TP / (TP + FN) if (TP + FN) != 0 else 0.0\n",
    "\n",
    "# Check for division by zero before calculating F1\n",
    "if (Precision + Recall) != 0:\n",
    "    F1 = 2 * Precision * Recall / (Precision + Recall)\n",
    "else:\n",
    "    F1 = 0.0\n",
    "\n",
    "# Handle division by zero for Specificity if TN + FP is 0\n",
    "Specificity = TN / (TN + FP) if (TN + FP) != 0 else 0.0\n",
    "\n",
    "print(f\"Accuracy = {Accuracy:.4f} ({Accuracy*100:.2f}%)\")\n",
    "print(f\"Precision = {Precision:.4f} ({Precision*100:.2f}%)\")\n",
    "print(f\"Recall = {Recall:.4f} ({Recall*100:.2f}%)\")\n",
    "print(f\"F1 Score = {F1:.4f} ({F1*100:.2f}%)\")\n",
    "print(f\"Specificity = {Specificity:.4f} ({Specificity*100:.2f}%)\\n\")\n",
    "\n",
    "print(\"Why is Accuracy misleading in this case?\")\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Accuracy is calculated as (TP + TN) / Total. In this scenario, the model predicts everything as negative. Since 98% of the data are actual negatives, the model correctly predicts 9800 out of 10000 samples. This leads to an accuracy of 98%. However, the model completely fails to identify any of the positive samples (Recall = 0), making it useless for detecting the minority class. A high accuracy in such an imbalanced dataset can give a false sense of good performance.\")\n",
    "\n",
    "print(\"\\nWhich metric would be more appropriate for imbalanced data and why?\")\n",
    "print(\"------------------------------------------------------------------\")\n",
    "print(\"For imbalanced datasets, metrics that specifically evaluate the model's performance on the minority class are more appropriate. These include:\")\n",
    "print(\"1. Recall (Sensitivity): This measures the proportion of actual positive cases that are correctly identified. In this example, Recall is 0, correctly highlighting the model's inability to detect positives.\")\n",
    "print(\"2. Precision: This measures the proportion of positive predictions that are actually correct. In this example, Precision is 0, as there are no correct positive predictions.\")\n",
    "print(\"3. F1-Score: This is the harmonic mean of Precision and Recall. It provides a balanced measure of a model's performance on positive cases, especially when there's an uneven class distribution. An F1-Score of 0 in this case correctly indicates poor performance.\")\n",
    "print(\"4. Specificity: This measures the proportion of actual negative cases that are correctly identified. While high here, it doesn't tell us about positive identification.\")\n",
    "print(\"5. Area Under the Receiver Operating Characteristic (ROC-AUC) curve or Precision-Recall (PR-AUC) curve: These provide a comprehensive evaluation of a classifier's performance across various classification thresholds, which is very useful for imbalanced datasets.\")\n",
    "print(\"\\nIn this specific case, metrics like Recall, Precision, and F1-Score clearly show the model's complete failure to identify the positive class, unlike accuracy which suggests a high performance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fb0c2c-72e2-431f-a094-27bc3c4d0552",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
